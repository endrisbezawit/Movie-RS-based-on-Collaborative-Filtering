{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Alternating Least Square E1.ipynb","provenance":[],"authorship_tag":"ABX9TyMsPluK75tmlFJswOtoVKIB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QENzUbDFyqc5","executionInfo":{"status":"ok","timestamp":1652921649462,"user_tz":-180,"elapsed":24870,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}},"outputId":"e722c330-aa5e-4aa2-a9a0-2d692244545f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Mounting drive to google colab\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!apt-get install openjdk-8-jdk-headless -qq > /dev/null"],"metadata":{"id":"lSenbJ_Bztn3","executionInfo":{"status":"ok","timestamp":1652921671891,"user_tz":-180,"elapsed":13540,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["!wget -q ftp://mirror.klaus-uwe.me/apache/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz"],"metadata":{"id":"FcTQrQSKzzE1","executionInfo":{"status":"ok","timestamp":1652921710615,"user_tz":-180,"elapsed":705,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["!tar xf spark-2.4.7-bin-hadoop2.7.tgz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ySYTs8zTz8Pl","executionInfo":{"status":"ok","timestamp":1652921719184,"user_tz":-180,"elapsed":514,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}},"outputId":"508044c9-5752-47da-97db-a64e34e20b4d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["tar: spark-2.4.7-bin-hadoop2.7.tgz: Cannot open: No such file or directory\n","tar: Error is not recoverable: exiting now\n"]}]},{"cell_type":"code","source":["!pip install -q findspark"],"metadata":{"id":"0PdGCfCJz_na","executionInfo":{"status":"ok","timestamp":1652921742136,"user_tz":-180,"elapsed":3569,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["!pip install py4j"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZfHs2_RQ0JTj","executionInfo":{"status":"ok","timestamp":1652921774845,"user_tz":-180,"elapsed":3485,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}},"outputId":"8c4a475f-3752-4c65-e1c2-5871a9bb335d"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting py4j\n","  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n","\u001b[?25l\r\u001b[K     |█▋                              | 10 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 20 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████                           | 30 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 40 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 51 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 61 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 71 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 81 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 92 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 102 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 112 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 122 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 133 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 143 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 153 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 163 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 174 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 184 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 194 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 199 kB 13.7 MB/s \n","\u001b[?25hInstalling collected packages: py4j\n","Successfully installed py4j-0.10.9.5\n"]}]},{"cell_type":"code","source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\""],"metadata":{"id":"4pKWdikq2Mjz","executionInfo":{"status":"ok","timestamp":1652922319016,"user_tz":-180,"elapsed":349,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["import findspark\n","findspark.find() #To find folder of SPARK HOME"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"xlZ6nFp12Uf6","executionInfo":{"status":"ok","timestamp":1652922340695,"user_tz":-180,"elapsed":389,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}},"outputId":"e1850d89-776a-4609-a440-dd3d86efea00"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/spark-2.4.7-bin-hadoop2.7'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["import findspark\n"],"metadata":{"id":"bItPXjm62XFy","executionInfo":{"status":"ok","timestamp":1652922620921,"user_tz":-180,"elapsed":344,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["!pip install pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g9M09c3N3uHc","executionInfo":{"status":"ok","timestamp":1652922762203,"user_tz":-180,"elapsed":54745,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}},"outputId":"32f6d301-832b-44a2-87ee-900fdca2720d"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n","\u001b[K     |████████████████████████████████| 281.4 MB 33 kB/s \n","\u001b[?25hCollecting py4j==0.10.9.3\n","  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n","\u001b[K     |████████████████████████████████| 198 kB 36.5 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=81c6b7d06c25c60aa5d2ba18fc6de20e3cd2ff0d7af9db4be62979f6a1407f3a\n","  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","  Attempting uninstall: py4j\n","    Found existing installation: py4j 0.10.9.5\n","    Uninstalling py4j-0.10.9.5:\n","      Successfully uninstalled py4j-0.10.9.5\n","Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"]}]},{"cell_type":"code","source":["import findspark\n","# findspark.find() =To find folder of SPARK HOME\n","findspark.init(\"/content/spark-2.4.7-bin-hadoop2.7\")# SPARK_HOME"],"metadata":{"id":"2V-E5eMR4D7L","executionInfo":{"status":"ok","timestamp":1652922811254,"user_tz":-180,"elapsed":358,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["!pip install pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YJ4JL-y3Bp4E","executionInfo":{"status":"ok","timestamp":1652925314954,"user_tz":-180,"elapsed":3350,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}},"outputId":"78747f3e-066e-4108-9559-ad528be17cea"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.1)\n","Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.3)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from pyspark.sql.functions import col, explode\n","from pyspark import SparkContext"],"metadata":{"id":"Q3T3jPwl3k42","executionInfo":{"status":"ok","timestamp":1652925367888,"user_tz":-180,"elapsed":332,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"PIhzyq55Bo99"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","sc = SparkContext\n","spark = SparkSession.builder.appName('rec_sys').getOrCreate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"id":"B-IzFOXo4MKa","executionInfo":{"status":"error","timestamp":1652925403732,"user_tz":-180,"elapsed":370,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}},"outputId":"f3e4588f-762f-41b0-c360-ba2e950b2057"},"execution_count":37,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-256dc1f00f87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rec_sys'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIG_IGN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mpopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preexec_fn'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreexec_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;31m# preexec_fn not supported on Windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    798\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    801\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1549\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/spark-2.4.7-bin-hadoop2.7/./bin/spark-submit': '/content/spark-2.4.7-bin-hadoop2.7/./bin/spark-submit'"]}]}]}