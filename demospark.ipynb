{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"demospark.ipynb","provenance":[],"authorship_tag":"ABX9TyM+vpunZ0zFRZlYy6364sRr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n"],"metadata":{"id":"KLZVuxstAs9s","executionInfo":{"status":"ok","timestamp":1649720638758,"user_tz":-180,"elapsed":2296,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"a9JOSAG9A0v8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ehX--mGen4Hc","executionInfo":{"status":"ok","timestamp":1649720360948,"user_tz":-180,"elapsed":10413,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}},"outputId":"4f683fd6-6f66-4043-e682-0029b7a6268b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.1)\n","Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.3)\n","ERROR: unknown command \"установить\"\n"]}],"source":["!pip install pyspark\n","!pip установить pyspark\n"]},{"cell_type":"code","source":[""],"metadata":{"id":"lLtY6YK1V0sl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!apt update\n","\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q https://downloads.apache.org/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz\n","!tar xf spark-2.4.8-bin-hadoop2.7.tgz\n","!pip install -q findspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_SUam3jwqJ-6","executionInfo":{"status":"ok","timestamp":1649720453974,"user_tz":-180,"elapsed":29598,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}},"outputId":"18a9d877-ecf9-44bb-83e0-015aa932bf49"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n","Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Ign:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,941 kB]\n","Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [993 kB]\n","Fetched 3,202 kB in 5s (691 kB/s)\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","40 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"f81w9bXmAsIL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-1.8.0-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.8-bin-hadoop2.7\"\n","\n"],"metadata":{"id":"yC_I1PUYqrDH","executionInfo":{"status":"ok","timestamp":1649720780544,"user_tz":-180,"elapsed":285,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["os.environ[\"SPARK_HOME\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"MgS9QQf8BNAT","executionInfo":{"status":"ok","timestamp":1649720746493,"user_tz":-180,"elapsed":286,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}},"outputId":"0be1361e-0b1a-4933-b9d4-f9651017d0b1"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/spark-2.4.8-bin-hadoop2.7'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["!apt update\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q https://downloads.apache.org/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz\n","!tar xf spark-2.4.8-bin-hadoop2.7.tgz\n","!pip install -q findspark\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-1.8.0-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.8-bin-hadoop2.7\"\n","import findspark\n","findspark.init()\n","import pyspark\n","# import necessary libraries\n","from pyspark import SparkContext\n","from pyspark.sql import SparkSession\n","# instantiate SparkSession object\n","spark = SparkSession.builder.master(\"local\").getOrCreate()\n","# read in the dataset into pyspark DataFrame\n","movie_ratings = spark.read.csv('/content/ratings.csv', header='true', inferSchema='true')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1UfZwoAnBUhZ","executionInfo":{"status":"ok","timestamp":1649721185595,"user_tz":-180,"elapsed":27573,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}},"outputId":"7328fce2-2df5-41a6-81b6-c9eb4b5934a2"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n","\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Connecting to security.\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (185.125.190.36\u001b[0m\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (185.125.190.36\u001b[0m\r                                                                               \rHit:3 http://security.ubuntu.com/ubuntu bionic-security InRelease\n","\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (185.125.190.36\u001b[0m\r                                                                               \rIgn:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (185.125.190.36\u001b[0m\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n","Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n","Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","40 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"]}]},{"cell_type":"code","source":["movie_ratings. show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qkAkHW1CYI2O","executionInfo":{"status":"ok","timestamp":1649726944113,"user_tz":-180,"elapsed":267,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}},"outputId":"c371b4e5-7ab6-4f89-e35a-450f76bb8318"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["+------+-------+------+---------+\n","|userId|movieId|rating|timestamp|\n","+------+-------+------+---------+\n","|     1|      1|   4.0|964982703|\n","|     1|      3|   4.0|964981247|\n","|     1|      6|   4.0|964982224|\n","|     1|     47|   5.0|964983815|\n","|     1|     50|   5.0|964982931|\n","|     1|     70|   3.0|964982400|\n","|     1|    101|   5.0|964980868|\n","|     1|    110|   4.0|964982176|\n","|     1|    151|   5.0|964984041|\n","|     1|    157|   5.0|964984100|\n","|     1|    163|   5.0|964983650|\n","|     1|    216|   5.0|964981208|\n","|     1|    223|   3.0|964980985|\n","|     1|    231|   5.0|964981179|\n","|     1|    235|   4.0|964980908|\n","|     1|    260|   5.0|964981680|\n","|     1|    296|   3.0|964982967|\n","|     1|    316|   3.0|964982310|\n","|     1|    333|   5.0|964981179|\n","|     1|    349|   4.0|964982563|\n","+------+-------+------+---------+\n","only showing top 20 rows\n","\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"OyP4-v6_-XXK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n","from pyspark.ml.recommendation import ALS\n","# split into training and testing sets\n","(training, test) = movie_ratings.randomSplit([.8, .2])\n","# Build the recommendation model using ALS on the training data\n","# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n","als = ALS(maxIter=5, rank=4, regParam=0.01, userCol='userId', itemCol='movieId', ratingCol='rating', coldStartStrategy='drop')\n","# fit the ALS model to the training set\n","model=als.fit(training)\n","# Evaluate the model by computing the RMSE on the test data\n","predictions = model.transform(test)\n","evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n","rmse = evaluator.evaluate(predictions)\n","print(rmse)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jXnaWTJkwYue","executionInfo":{"status":"ok","timestamp":1649721216965,"user_tz":-180,"elapsed":8146,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}},"outputId":"4bc6f64c-54e0-41c1-f7f8-0db6217b5bf4"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["1.0001948928821516\n"]}]},{"cell_type":"code","source":["from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n","# initialize the ALS model\n","als_model = ALS(userCol='userId', itemCol='movieId', ratingCol='rating', coldStartStrategy='drop')\n","# create the parameter grid\n","params = ParamGridBuilder().addGrid(als_model.regParam, [.01, .05, .1, .15]).addGrid(als_model.rank, [10, 50, 100, 150]).build()\n","#instantiating crossvalidator estimator\n","cv = CrossValidator(estimator=als_model, estimatorParamMaps=params, evaluator=evaluator, parallelism=4)\n","best_model = cv.fit(movie_ratings)\n","model = best_model.bestModel"],"metadata":{"id":"crlIJ-7Zwqq-","executionInfo":{"status":"ok","timestamp":1649723643690,"user_tz":-180,"elapsed":2387604,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["final_als = ALS(maxIter=10, rank=50, regParam=0.15, userCol='userId', itemCol='movieId', ratingCol='rating', coldStartStrategy='drop')\n","final_model = final_als.fit(training)\n","test_predictions = final_model.transform(test)\n","RMSE = evaluator.evaluate(test_predictions)\n","print(RMSE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SfRsFxvw0ges","executionInfo":{"status":"ok","timestamp":1649725624768,"user_tz":-180,"elapsed":17750,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}},"outputId":"b6f2283a-5d51-468b-af19-509023f271f9"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8711738157859104\n"]}]},{"cell_type":"code","source":["movie_titles = spark.read.csv('movies.csv', header='true', inferSchema='true')"],"metadata":{"id":"wFrheTn3T9Gp","executionInfo":{"status":"ok","timestamp":1649725665937,"user_tz":-180,"elapsed":631,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["def name_retriever(movie_id, movie_title_df):\n","    return movie_title_df.where(movie_title_df.movieId == movie_id).take(1)[0]['title']"],"metadata":{"id":"iDdUeXTgZDvK","executionInfo":{"status":"ok","timestamp":1649727022592,"user_tz":-180,"elapsed":276,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["def new_user_recs(user_id, rating_df, movie_title_df, num_ratings, num_recs):\n","\n","  input_samples = rating_df.sample(False, 0.001, seed=100).collect()\n","  sample_list = [i[1] for i in input_samples]\n","  new_ratings = []\n","  for i in range(len(sample_list)):\n","    print(name_retriever(sample_list[i], movie_title_df))\n","    rating = input('How do you rate this movie on a scale of 1-5, press n if you have not seen :\\n')\n","    if rating == 'n':\n","      continue\n","    else:\n","      new_ratings.append((user_id, sample_list[i], int(rating)))\n","      num_ratings -= 1\n","      if num_ratings == 0:\n","        break\n","        \n","    # turn the new_recommendations list into a spark DataFrame\n","  new_user_ratings = spark.createDataFrame(new_ratings, rating_df.columns)\n","    \n","    # combine the new ratings df with the rating_df\n","  movie_ratings_combined = rating_df.union(new_user_ratings)\n","    \n","    # create an ALS model and fit it\n","  als = ALS(maxIter=10, rank=50, regParam=0.15, userCol='userId', itemCol='movieId', ratingCol='rating', coldStartStrategy='drop')\n","  model = als.fit(movie_ratings_combined)\n","    \n","    # make recommendations for all users using the recommendForAllUsers method\n","  recommendations = model.recommendForAllUsers(num_recs)\n","    \n","    # get recommendations specifically for the new user that has been added to the DataFrame\n","  recs_for_user = recommendations.where(recommendations.userId == user_id).take(1)\n","\n","  for ranking, (movie_id, rating) in enumerate(recs_for_user[0]['recommendations']):\n","    movie_string = name_retriever(movie_id, movie_title_df)\n","    print('Recommendation {}: {} | predicted score: {}'.format(ranking+1, movie_string, rating))"],"metadata":{"id":"lpqa4Aw1ZP3A","executionInfo":{"status":"ok","timestamp":1649727058547,"user_tz":-180,"elapsed":241,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["new_user_recs(1000, movie_ratings, movie_titles, 5, 10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":588},"id":"mO2WiwX4Zqiw","executionInfo":{"status":"error","timestamp":1649727379890,"user_tz":-180,"elapsed":18772,"user":{"displayName":"dagmbeza Endris","userId":"08980705504316818936"}},"outputId":"b96b1f7f-0481-4611-819e-c0b1c87943b9"},"execution_count":38,"outputs":[{"name":"stdout","output_type":"stream","text":["Field of Dreams (1989)\n","How do you rate this movie on a scale of 1-5, press n if you have not seen :\n","1\n","Edward Scissorhands (1990)\n","How do you rate this movie on a scale of 1-5, press n if you have not seen :\n","2\n","Atlantis: The Lost Empire (2001)\n","How do you rate this movie on a scale of 1-5, press n if you have not seen :\n","3\n","Prestige, The (2006)\n","How do you rate this movie on a scale of 1-5, press n if you have not seen :\n","4\n","Felon (2008)\n","How do you rate this movie on a scale of 1-5, press n if you have not seen :\n","5\n"]},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-db9ec8d88ccf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_user_recs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_ratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_titles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-35-1d8ed7e55208>\u001b[0m in \u001b[0;36mnew_user_recs\u001b[0;34m(user_id, rating_df, movie_title_df, num_ratings, num_recs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# turn the new_recommendations list into a spark DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mnew_user_ratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_ratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrating_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# combine the new ratings df with the rating_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.4.8-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.4.8-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                     \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m                     \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]}]}